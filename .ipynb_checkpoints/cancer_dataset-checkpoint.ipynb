{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a4b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw\n",
    "from praw_credential import reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c8ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset creator\n",
    "'''\n",
    "\n",
    "def subreddit_scrapper(subject):\n",
    "    # needed librairies \n",
    "    import pandas as pd\n",
    "    import praw\n",
    "    import datetime\n",
    "    #Connecting to the api\n",
    "    from praw_credential import reddit\n",
    "    \n",
    "    #defining subject\n",
    "    top_post = []\n",
    "    subreddit = reddit.subreddit(subject)\n",
    "    for post in subreddit.top(limit=None):\n",
    "        top_post.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "    \n",
    "    #defining the features    \n",
    "    top_posts = pd.DataFrame(top_post,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    "    \n",
    "    #adjusting the created_date\n",
    "    top_posts['created'] = pd.to_datetime(top_posts['created'], unit='s')\n",
    "    top_posts.created = top_posts.created.dt.tz_localize('UTC')\n",
    "    \n",
    "    #Adding UID as index\n",
    "    top_posts['UID'] = range(1, 1+len(top_posts))\n",
    "    \n",
    "    #ignoring covid wording\n",
    "    df = top_posts[top_posts[\"title\"].str.contains(\"COVID|corona|Covid|coronavirus|Coronavirus|covid\")==False]\n",
    "    \n",
    "    #Returning the dataframe:\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "comment scrapper \n",
    "'''\n",
    "\n",
    "def reddit_scrapper(subject):\n",
    "    from praw.models import MoreComments\n",
    "    import numpy as np\n",
    "    #Connecting to the api\n",
    "    from praw_credential import reddit\n",
    "    \n",
    "    #definig df:\n",
    "    df = subreddit_scrapper(subject)\n",
    "    df['body'].replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=['body'], inplace=True)\n",
    "       \n",
    "    subreddit = reddit.subreddit(subject)\n",
    "    #post lists\n",
    "    posts = []\n",
    "    # comments in dict\n",
    "    title_comment_dic = {}\n",
    "    \n",
    "    #iterating in all subreddits and assign to column body\n",
    "    for submission in subreddit.top('all'):\n",
    "        posts = []\n",
    "        for top_level_comment in submission.comments:\n",
    "            if isinstance(top_level_comment, MoreComments):\n",
    "                continue\n",
    "            posts.append(top_level_comment.body)\n",
    "        postdf = pd.DataFrame(posts,columns=[\"body\"])\n",
    "        indexNames = postdf[(postdf.body == '[removed]') | (postdf.body == '[deleted]')].index\n",
    "        postdf.drop(indexNames, inplace=True)\n",
    "        uid = df['UID'][df['title'] == submission.title].values\n",
    "        if len(uid) > 0:\n",
    "            title_comment_dic[uid[0]] = postdf\n",
    "    \n",
    "    #Returning the comment dictionary \n",
    "    return title_comment_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "comment scrapper \n",
    "'''\n",
    "\n",
    "def reddit_scrapper_id(pointer):\n",
    "    from praw.models import MoreComments\n",
    "    submission = reddit.submission(id = pointer)\n",
    "    posts = []\n",
    "    for top_level_comment in submission.comments[1:]:\n",
    "        if isinstance(top_level_comment, MoreComments):\n",
    "            continue\n",
    "        posts.append(top_level_comment.body)\n",
    "\n",
    "    posts = pd.DataFrame(posts,columns=[\"body\"])\n",
    "    indexNames = posts[(posts.body == '[removed]') | (posts.body == '[deleted]')].index\n",
    "    posts.drop(indexNames, inplace=True)\n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5571847",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Merged with comment\n",
    "'''\n",
    "def merger(subject):\n",
    "    # first let's convert the myscrapping dictionary into pandas dataframe\n",
    "    df_scrapped = pd.DataFrame.from_dict(reddit_scrapper(subject), orient='index')\n",
    "    df_scrapped.columns = ['comments']\n",
    "    df_scrapped.index.name = 'UID'\n",
    "    \n",
    "    \n",
    "    return df_scrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9124eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Return comments as dataframe\n",
    "'''\n",
    "\n",
    "def return_comments(subject):\n",
    "    #mydata\n",
    "    data = subreddit_scrapper(subject)\n",
    "    \n",
    "    #Pull up id's\n",
    "    ids = data.id.values.tolist()\n",
    "    # init a dict\n",
    "    myscrapping = {}\n",
    "\n",
    "    # Scrap them all\n",
    "    for i in ids:\n",
    "        myscrapping[i] = reddit_scrapper_id(i)\n",
    "    \n",
    "    #first let's convert the myscrapping dictionary into pandas dataframe\n",
    "    comments  = pd.DataFrame.from_dict(myscrapping, orient='index')\n",
    "    comments.columns = ['comments']\n",
    "    comments.index.name = 'id'\n",
    "    \n",
    "    # return dictionary\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the return comments\n",
    "\n",
    "comments = return_comments('cancer')\n",
    "comments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491e226",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d69cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Main function\n",
    "'''\n",
    "\n",
    "def main(subject):\n",
    "    # now defining a list of frames \n",
    "    import numpy as np\n",
    "    \n",
    "    #definig df:\n",
    "    df_prawing = subreddit_scrapper(subject)\n",
    "    df_prawing['body'].replace('', np.nan, inplace=True)\n",
    "    df_prawing.dropna(subset=['body'], inplace=True)\n",
    "    \n",
    "    df_comments = merger(subject)\n",
    "    \n",
    "    #a list of frames\n",
    "    frames = [df_prawing , df_comments]\n",
    "    \n",
    "    #joining \n",
    "    #df = pd.concat(frames) will not work!\n",
    "    df = df_prawing.merge(df_comments, how = 'inner', on = ['UID'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = main('cancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e6888f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
