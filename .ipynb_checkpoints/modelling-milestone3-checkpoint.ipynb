{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55a4389",
   "metadata": {},
   "source": [
    "# Examination of Digital Community Conversations Within Specific Disease States Via Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc7424f",
   "metadata": {},
   "source": [
    "- **Vision**: Development of a repeatable process for the analysis of Reddit conversations\n",
    "within specific condition and/or disease state with applicable threads and subreddit\n",
    "threads (subreddits) to potentially inform strategy and content development. Create a\n",
    "simplified and repeatable process that does not require the users to be fluent in Reddit.\n",
    "- **Issue**: While Reddit offers robust, open, and community-minded discussions surrounding\n",
    "conditions and disease states, Reddit also provides volumes of unstructured and\n",
    "unclassified data. The development of a repeatable process – that continues to monitor\n",
    "evolving conversations over time – currently requires multiple tools (ex. – tools to scrape\n",
    "threads, tools to analyze keyword content, tools to analyze sentiment, etc.).\n",
    "- **Method**: After identifying priority conditions and/or disease states with active Reddit\n",
    "communities (ex. – prostate cancer, breast cancer, HIV, etc.), build relational taxonomy\n",
    "(ex. – medicine, treatment, and adherence all have specific topics but have relational\n",
    "discussions) of topical themes addressed within.\n",
    "- **Potential Output**: Provide use case for healthcare companies on the importance of\n",
    "Reddit as an early source of social indicator of trends and conversational “lexicon” to be\n",
    "used for patient communications and programs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a435c5c",
   "metadata": {},
   "source": [
    "# <font color='Green'><center>Milestone 3: Modelling</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aff1c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='img/logo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd5ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Needed librairies\n",
    "'''\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Data analysis and Wrangling\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "from datetime import date, datetime, time, timedelta\n",
    "import datetime as dt\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import matplotlib.ticker as mtic\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Sentiment Analysis: NLP Packages\n",
    "from textblob import TextBlob\n",
    "\n",
    "from gensim import *\n",
    "\n",
    "# Iextras\n",
    "\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "239ee551",
   "metadata": {},
   "source": [
    "'''\n",
    "Importing the data from: dataset-milestone1\n",
    "'''\n",
    "#from dataset_milestone1 import df\n",
    "from cancer_dataset import cancer as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd898bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "      <th>UID</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’m not ready.</td>\n",
       "      <td>632</td>\n",
       "      <td>9isza1</td>\n",
       "      <td>cancer</td>\n",
       "      <td>https://www.reddit.com/r/cancer/comments/9isza...</td>\n",
       "      <td>82</td>\n",
       "      <td>In 2016 I was diagnosed with stage 4 of a seve...</td>\n",
       "      <td>2018-09-25 15:13:21+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I’ve got 2-4 weeks to live, we’re in the end g...</td>\n",
       "      <td>590</td>\n",
       "      <td>m1h5h5</td>\n",
       "      <td>cancer</td>\n",
       "      <td>https://www.reddit.com/r/cancer/comments/m1h5h...</td>\n",
       "      <td>149</td>\n",
       "      <td>Hey everyone you might have seen my post on he...</td>\n",
       "      <td>2021-03-09 21:42:48+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Onto Hospice. End of journey.</td>\n",
       "      <td>458</td>\n",
       "      <td>8y27xr</td>\n",
       "      <td>cancer</td>\n",
       "      <td>https://www.reddit.com/r/cancer/comments/8y27x...</td>\n",
       "      <td>108</td>\n",
       "      <td>The last treatment option, Immunotherapy, for ...</td>\n",
       "      <td>2018-07-11 18:41:20+00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diagnosed in June, Dead in August</td>\n",
       "      <td>438</td>\n",
       "      <td>cuo28h</td>\n",
       "      <td>cancer</td>\n",
       "      <td>https://www.reddit.com/r/cancer/comments/cuo28...</td>\n",
       "      <td>49</td>\n",
       "      <td>My wonderful husband was diagnosed with cancer...</td>\n",
       "      <td>2019-08-24 03:00:29+00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Goodbye my sweet angel. I Lost my 5 year old d...</td>\n",
       "      <td>441</td>\n",
       "      <td>e1o110</td>\n",
       "      <td>cancer</td>\n",
       "      <td>https://www.reddit.com/r/cancer/comments/e1o11...</td>\n",
       "      <td>47</td>\n",
       "      <td>We had an incredible six months together after...</td>\n",
       "      <td>2019-11-25 22:31:46+00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>I am getting a Bone Marrow Transplant tonight!!!</td>\n",
       "      <td>204</td>\n",
       "      <td>asu1lg</td>\n",
       "      <td>cancer</td>\n",
       "      <td>https://www.reddit.com/r/cancer/comments/asu1l...</td>\n",
       "      <td>67</td>\n",
       "      <td>Please give me those good reddit vibes. Quick ...</td>\n",
       "      <td>2019-02-20 21:29:55+00:00</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>They say you die twice: once when you pass, an...</td>\n",
       "      <td>207</td>\n",
       "      <td>n9fmjd</td>\n",
       "      <td>cancer</td>\n",
       "      <td>https://www.reddit.com/r/cancer/comments/n9fmj...</td>\n",
       "      <td>29</td>\n",
       "      <td>Maybe I'm posting this for my own benefit, but...</td>\n",
       "      <td>2021-05-10 20:46:10+00:00</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Today is my 10th Cancerversay - diagnosed with...</td>\n",
       "      <td>203</td>\n",
       "      <td>jlosty</td>\n",
       "      <td>cancer</td>\n",
       "      <td>https://www.reddit.com/r/cancer/comments/jlost...</td>\n",
       "      <td>35</td>\n",
       "      <td>I (38M) got my biopsy results 10 years ago on ...</td>\n",
       "      <td>2020-10-31 19:46:51+00:00</td>\n",
       "      <td>98</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Hi, I'm NED!!</td>\n",
       "      <td>204</td>\n",
       "      <td>l7xc0p</td>\n",
       "      <td>cancer</td>\n",
       "      <td>https://www.reddit.com/r/cancer/comments/l7xc0...</td>\n",
       "      <td>44</td>\n",
       "      <td>Last year I was diagnosed with Stage 4 bowel c...</td>\n",
       "      <td>2021-01-29 16:46:41+00:00</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Male cancer nurse here</td>\n",
       "      <td>199</td>\n",
       "      <td>jkk09e</td>\n",
       "      <td>cancer</td>\n",
       "      <td>https://www.reddit.com/r/cancer/comments/jkk09...</td>\n",
       "      <td>36</td>\n",
       "      <td>I have a patient. 30M with Metastatic Rhabdomy...</td>\n",
       "      <td>2020-10-29 22:10:21+00:00</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  score      id  \\\n",
       "0                                      I’m not ready.    632  9isza1   \n",
       "1   I’ve got 2-4 weeks to live, we’re in the end g...    590  m1h5h5   \n",
       "2                       Onto Hospice. End of journey.    458  8y27xr   \n",
       "3                   Diagnosed in June, Dead in August    438  cuo28h   \n",
       "4   Goodbye my sweet angel. I Lost my 5 year old d...    441  e1o110   \n",
       "..                                                ...    ...     ...   \n",
       "66   I am getting a Bone Marrow Transplant tonight!!!    204  asu1lg   \n",
       "67  They say you die twice: once when you pass, an...    207  n9fmjd   \n",
       "68  Today is my 10th Cancerversay - diagnosed with...    203  jlosty   \n",
       "69                                      Hi, I'm NED!!    204  l7xc0p   \n",
       "70                             Male cancer nurse here    199  jkk09e   \n",
       "\n",
       "   subreddit                                                url  num_comments  \\\n",
       "0     cancer  https://www.reddit.com/r/cancer/comments/9isza...            82   \n",
       "1     cancer  https://www.reddit.com/r/cancer/comments/m1h5h...           149   \n",
       "2     cancer  https://www.reddit.com/r/cancer/comments/8y27x...           108   \n",
       "3     cancer  https://www.reddit.com/r/cancer/comments/cuo28...            49   \n",
       "4     cancer  https://www.reddit.com/r/cancer/comments/e1o11...            47   \n",
       "..       ...                                                ...           ...   \n",
       "66    cancer  https://www.reddit.com/r/cancer/comments/asu1l...            67   \n",
       "67    cancer  https://www.reddit.com/r/cancer/comments/n9fmj...            29   \n",
       "68    cancer  https://www.reddit.com/r/cancer/comments/jlost...            35   \n",
       "69    cancer  https://www.reddit.com/r/cancer/comments/l7xc0...            44   \n",
       "70    cancer  https://www.reddit.com/r/cancer/comments/jkk09...            36   \n",
       "\n",
       "                                                 body  \\\n",
       "0   In 2016 I was diagnosed with stage 4 of a seve...   \n",
       "1   Hey everyone you might have seen my post on he...   \n",
       "2   The last treatment option, Immunotherapy, for ...   \n",
       "3   My wonderful husband was diagnosed with cancer...   \n",
       "4   We had an incredible six months together after...   \n",
       "..                                                ...   \n",
       "66  Please give me those good reddit vibes. Quick ...   \n",
       "67  Maybe I'm posting this for my own benefit, but...   \n",
       "68  I (38M) got my biopsy results 10 years ago on ...   \n",
       "69  Last year I was diagnosed with Stage 4 bowel c...   \n",
       "70  I have a patient. 30M with Metastatic Rhabdomy...   \n",
       "\n",
       "                      created  UID  \\\n",
       "0   2018-09-25 15:13:21+00:00    2   \n",
       "1   2021-03-09 21:42:48+00:00    3   \n",
       "2   2018-07-11 18:41:20+00:00    9   \n",
       "3   2019-08-24 03:00:29+00:00   10   \n",
       "4   2019-11-25 22:31:46+00:00   11   \n",
       "..                        ...  ...   \n",
       "66  2019-02-20 21:29:55+00:00   94   \n",
       "67  2021-05-10 20:46:10+00:00   95   \n",
       "68  2020-10-31 19:46:51+00:00   98   \n",
       "69  2021-01-29 16:46:41+00:00   99   \n",
       "70  2020-10-29 22:10:21+00:00  100   \n",
       "\n",
       "                                             comments  \n",
       "0                                                 ...  \n",
       "1                                                 ...  \n",
       "2                                                 ...  \n",
       "3                                                 ...  \n",
       "4                                                 ...  \n",
       "..                                                ...  \n",
       "66                                                ...  \n",
       "67                                                ...  \n",
       "68                                                ...  \n",
       "69                                                ...  \n",
       "70                                                ...  \n",
       "\n",
       "[71 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('dataset/cancer.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1263cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df  = df.comments.iloc[:][1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "397f86bc",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "\n",
    "reddit = praw.Reddit(client_id='5PtLw2OXKn8K-lTmDq8WaA',\n",
    "     client_secret='jxi9Ob2bO6axkXP_eKKQH4tFO5t9Rg',\n",
    "     username='raymastouri',\n",
    "     password='M@$t0ur1198700rq',\n",
    "     user_agent='PatientCom/0.0.1'\n",
    ")\n",
    "\n",
    "top_post = []\n",
    "subreddit = reddit.subreddit('cancer')\n",
    "for post in subreddit.top(limit=None):\n",
    "    comments = []\n",
    "    for comment in post.comments:\n",
    "        if isinstance(comment, MoreComments):\n",
    "            continue        \n",
    "        comments.append(comment.body)\n",
    "    top_post.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,comments])\n",
    "top_posts = pd.DataFrame(top_post,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created','comments'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa944b0c",
   "metadata": {},
   "source": [
    "top_posts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f90f3806",
   "metadata": {},
   "source": [
    "df = top_posts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b116b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Making copies for the 3 modeling tasks \n",
    "'''\n",
    "df_model1= df.copy()\n",
    "df_model2= df.copy()\n",
    "df_model3= df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af70132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65441cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I’m not ready.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to list\n",
    "import pprint\n",
    "\n",
    "df = df_model1.title.values.tolist()\n",
    "\n",
    "df = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in df]\n",
    "\n",
    "# Remove new line characters\n",
    "df = [re.sub('\\s+', ' ', sent) for sent in df]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "df = [re.sub(\"\\'\", \"\", sent) for sent in df]\n",
    "\n",
    "df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73ec9b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [re.sub(\"-\", \" \", sent) for sent in df]\n",
    "df = [re.sub(\":\", \"\", sent) for sent in df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b5d65b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "df_words = list(sent_to_words(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0980417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "\n",
    "bigram = gensim.models.Phrases(df_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[df_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e115401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc4696b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'simple_preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17980/3391383998.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Remove Stop Words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata_words_nostops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Form Bigrams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17980/2989821661.py\u001b[0m in \u001b[0;36mremove_stopwords\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_bigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbigram_mod\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17980/2989821661.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_bigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbigram_mod\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'simple_preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "\n",
    "data_words_nostops = remove_stopwords(df_words)\n",
    "\n",
    "# Form Bigrams\n",
    "\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40217b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e54987",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[10:11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a42da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad33ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8677454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus)) \n",
    "\n",
    "# Compute Coherence Score\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NLP Librairies\n",
    "'''\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.gensim \n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6abff78",
   "metadata": {},
   "source": [
    "# Topic modeling using NLTK and Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeb4c48",
   "metadata": {},
   "source": [
    "\n",
    "Popular topic modeling algorithms include Latent Semantic Analysis (LSA) a.k.a Latent Semantic Indexing , Hierarchical Dirichlet Process (HDP), Latent Dirichlet Allocation (LDA) and Non-negative Matrix factorization among which LDA has shown great results in practice and therefore widely adopted. We'll look at them all one by one. Lets get to it.\n",
    "\n",
    "    Latent Semantic Analysis\n",
    "    Latent Dirichlet Allocation\n",
    "    Hierarchical Dirichlet Process\n",
    "    Non-negative Matrix factorization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7417038",
   "metadata": {},
   "source": [
    "Stopwords are words that are commonly used. Using the popular NLTK package in python, lets import the stopwords in the english language\n",
    "and save it. It'll be used later for modeling purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use','a','about', 'above', 'across'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042d241e",
   "metadata": {},
   "source": [
    "The extended stopwords list from the scikit learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "st1= ['after', 'afterwards','again','against', 'all', 'almost','alone','along',\n",
    "           'already',\n",
    "           'also',\n",
    "           'although',\n",
    "           'always',\n",
    "           'am',\n",
    "           'among',\n",
    "           'amongst',\n",
    "           'amoungst',\n",
    "           'amount',\n",
    "           'an',\n",
    "           'and',\n",
    "           'another',\n",
    "           'any',\n",
    "           'anyhow',\n",
    "           'anyone',\n",
    "           'anything',\n",
    "           'anyway',\n",
    "           'anywhere',\n",
    "           'are',\n",
    "           'around',\n",
    "           'as',\n",
    "           'at',\n",
    "           'back',\n",
    "           'be',\n",
    "           'became',\n",
    "           'because',\n",
    "           'become',\n",
    "           'becomes',\n",
    "           'becoming',\n",
    "           'been',\n",
    "           'before',\n",
    "           'beforehand',\n",
    "           'behind',\n",
    "           'being',\n",
    "           'below',\n",
    "           'beside',\n",
    "           'besides',\n",
    "           'between',\n",
    "           'beyond',\n",
    "           'bill',\n",
    "           'both',\n",
    "           'bottom',\n",
    "           'but',\n",
    "           'by',\n",
    "           'call',\n",
    "           'can',\n",
    "           'cannot',\n",
    "           'cant',\n",
    "           'co',\n",
    "           'con',\n",
    "           'could',\n",
    "           'couldnt',\n",
    "           'cry',\n",
    "           'de',\n",
    "           'describe',\n",
    "           'detail',\n",
    "           'do',\n",
    "           'done',\n",
    "           'down',\n",
    "           'due',\n",
    "           'during',\n",
    "           'each',\n",
    "           'eg',\n",
    "           'eight',\n",
    "           'either',\n",
    "           'eleven',\n",
    "           'else',\n",
    "           'elsewhere',\n",
    "           'empty',\n",
    "           'enough',\n",
    "           'etc',\n",
    "           'even',\n",
    "           'ever',\n",
    "           'every',\n",
    "           'everyone',\n",
    "           'everything',\n",
    "           'everywhere',\n",
    "           'except',\n",
    "           'few',\n",
    "           'fifteen',\n",
    "           'fifty',\n",
    "           'fill',\n",
    "           'find',\n",
    "           'fire',\n",
    "           'first',\n",
    "           'five',\n",
    "           'for',\n",
    "           'former',\n",
    "           'formerly',\n",
    "           'forty',\n",
    "           'found',\n",
    "           'four',\n",
    "           'from',\n",
    "           'front',\n",
    "           'full',\n",
    "           'further',\n",
    "           'get',\n",
    "           'give',\n",
    "           'go',\n",
    "           'had',\n",
    "           'has',\n",
    "           'hasnt',\n",
    "           'have',\n",
    "           'he',\n",
    "           'hence',\n",
    "           'her',\n",
    "           'here',\n",
    "           'hereafter',\n",
    "           'hereby',\n",
    "           'herein',\n",
    "           'hereupon',\n",
    "           'hers',\n",
    "           'herself',\n",
    "           'him',\n",
    "           'himself',\n",
    "           'his',\n",
    "           'how',\n",
    "           'however',\n",
    "           'hundred',\n",
    "           'i',\n",
    "           'ie',\n",
    "           'if',\n",
    "           'in',\n",
    "           'inc',\n",
    "           'indeed',\n",
    "           'interest',\n",
    "           'into',\n",
    "           'is',\n",
    "           'it',\n",
    "           'its',\n",
    "           'itself',\n",
    "           'keep',\n",
    "           'last',\n",
    "           'latter',\n",
    "           'latterly',\n",
    "           'least',\n",
    "           'less',\n",
    "           'ltd',\n",
    "           'made',\n",
    "           'many',\n",
    "           'may',\n",
    "           'me',\n",
    "           'meanwhile',\n",
    "           'might',\n",
    "           'mill',\n",
    "           'mine',\n",
    "           'more',\n",
    "           'moreover',\n",
    "           'most',\n",
    "           'mostly',\n",
    "           'move',\n",
    "           'much',\n",
    "           'must',\n",
    "           'my',\n",
    "           'myself',\n",
    "           'name',\n",
    "           'namely',\n",
    "           'neither',\n",
    "           'never',\n",
    "           'nevertheless',\n",
    "           'next',\n",
    "           'nine',\n",
    "           'no',\n",
    "           'nobody',\n",
    "           'none',\n",
    "           'noone',\n",
    "           'nor',\n",
    "           'not',\n",
    "           'nothing',\n",
    "           'now',\n",
    "           'nowhere',\n",
    "           'of',\n",
    "           'off',\n",
    "           'often',\n",
    "           'on',\n",
    "           'once',\n",
    "           'one',\n",
    "           'only',\n",
    "           'onto',\n",
    "           'or',\n",
    "           'other',\n",
    "           'others',\n",
    "           'otherwise',\n",
    "           'our',\n",
    "           'ours',\n",
    "           'ourselves',\n",
    "           'out',\n",
    "           'over',\n",
    "           'own',\n",
    "           'part',\n",
    "           'per',\n",
    "           'perhaps',\n",
    "           'please',\n",
    "           'put',\n",
    "           'rather',\n",
    "           're',\n",
    "           'same',\n",
    "           'see',\n",
    "           'seem',\n",
    "           'seemed',\n",
    "           'seeming',\n",
    "           'seems',\n",
    "           'serious',\n",
    "           'several',\n",
    "           'she',\n",
    "           'should',\n",
    "           'show',\n",
    "           'side',\n",
    "           'since',\n",
    "           'sincere',\n",
    "           'six',\n",
    "           'sixty',\n",
    "           'so',\n",
    "           'some',\n",
    "           'somehow',\n",
    "           'someone',\n",
    "           'something',\n",
    "           'sometime',\n",
    "           'sometimes',\n",
    "           'somewhere',\n",
    "           'still',\n",
    "           'such',\n",
    "           'system',\n",
    "           'take',\n",
    "           'ten',\n",
    "           'than',\n",
    "           'that',\n",
    "           'the',\n",
    "           'their',\n",
    "           'them',\n",
    "           'themselves',\n",
    "           'then',\n",
    "           'thence',\n",
    "           'there',\n",
    "           'thereafter',\n",
    "           'thereby',\n",
    "           'therefore',\n",
    "           'therein',\n",
    "           'thereupon',\n",
    "           'these',\n",
    "           'they',\n",
    "           'thick',\n",
    "           'thin',\n",
    "           'third',\n",
    "           'this',\n",
    "           'those',\n",
    "           'though',\n",
    "           'three',\n",
    "           'through',\n",
    "           'throughout',\n",
    "           'thru',\n",
    "           'thus',\n",
    "           'to',\n",
    "           'together',\n",
    "           'too',\n",
    "           'top',\n",
    "           'toward',\n",
    "           'towards',\n",
    "           'twelve',\n",
    "           'twenty',\n",
    "           'two',\n",
    "           'un',\n",
    "           'under',\n",
    "           'until',\n",
    "           'up',\n",
    "           'upon',\n",
    "           'us',\n",
    "           'very',\n",
    "           'via',\n",
    "           'was',\n",
    "           'we',\n",
    "           'well',\n",
    "           'were',\n",
    "           'what',\n",
    "           'whatever',\n",
    "           'when',\n",
    "           'whence',\n",
    "           'whenever',\n",
    "           'where',\n",
    "           'whereafter',\n",
    "           'whereas',\n",
    "           'whereby',\n",
    "           'wherein',\n",
    "           'whereupon',\n",
    "           'wherever',\n",
    "           'whether',\n",
    "           'which',\n",
    "           'while',\n",
    "           'whither',\n",
    "           'who',\n",
    "           'whoever',\n",
    "           'whole',\n",
    "           'whom',\n",
    "           'whose',\n",
    "           'why',\n",
    "           'will',\n",
    "           'with',\n",
    "           'within',\n",
    "           'without',\n",
    "           'would',\n",
    "           'yet',\n",
    "           'you',\n",
    "           'your',\n",
    "           'yours',\n",
    "           'yourself',\n",
    "           'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend(st1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589da462",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "break out data from nested columns \n",
    "'''\n",
    "df['comments'].iloc[:][70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b0c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.comments.iloc[:][70]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf0fc2f",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d61ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds='tsne', sort_topics=True)\n",
    "\n",
    "\n",
    "\n",
    "topic_data =  pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds = 'pcoa', sort_topics=True)\n",
    "pyLDAvis.display(topic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f6024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78a7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=1, limit=6, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bda0b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=6; start=1; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c4f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde68f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "\n",
    "optimal_model = model_list[3]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "print(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea3df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "   \n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "   \n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # -- dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    \n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "    \n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=df)\n",
    "\n",
    "\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "\n",
    "df_dominant_topic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b80d0d",
   "metadata": {},
   "source": [
    "# HDP Model: Hierarchical Dirichlet Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca37d7",
   "metadata": {},
   "source": [
    "HDP is a nonparametric Bayesian approach to clustering grouped data. It uses a Dirichlet process for each group of data, with the Dirichlet processes for all groups sharing a base distribution which is itself drawn from a Dirichlet process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e7777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import CoherenceModel, HdpModel\n",
    "hdpmodel = HdpModel(corpus=corpus, id2word=id2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde8b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hdptopics = hdpmodel.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d56c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdptopics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98695ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hdptopics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c364b",
   "metadata": {},
   "source": [
    "# NMF model: Non-Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af3a2c1",
   "metadata": {},
   "source": [
    "It is a group of algorithms where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b1074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords;\n",
    "import nltk;\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer;\n",
    "from sklearn.decomposition import NMF;\n",
    "from sklearn.preprocessing import normalize;\n",
    "title = df_model1.title\n",
    "vectorizer = CountVectorizer(analyzer='word', max_features=5000, stop_words='english', lowercase=True, \n",
    "                             token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}');\n",
    "x_counts = vectorizer.fit_transform(title);\n",
    "print( \"Created %d X %d document-term matrix\" % (x_counts.shape[0], x_counts.shape[1]) )\n",
    "transformer = TfidfTransformer(smooth_idf=False);\n",
    "x_tfidf = transformer.fit_transform(x_counts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f0bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)\n",
    "model = NMF(n_components=5, init='nndsvd');\n",
    "model.fit(xtfidf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90abb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "nmf_df = get_nmf_topics(model, 5)\n",
    "nmf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49822a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = title.str.strip()\n",
    "raw_documents= raw_documents.str.lower()\n",
    "raw_documents = raw_documents.tolist()\n",
    "raw_doc1 = [i.split() for i in raw_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3094c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# use a custom stopwords list, set the minimum term-document frequency to 20\n",
    "vectorizer = CountVectorizer(stop_words = stop_words, min_df = 20) #custom_stop_words\n",
    "A = vectorizer.fit_transform(raw_documents)\n",
    "print( \"Created %d X %d document-term matrix\" % (A.shape[0], A.shape[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7804757",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# we can pass in the same preprocessing parameters\n",
    "vectorizer = TfidfVectorizer(stop_words= stop_words, min_df = 20) #custom_stop_words\n",
    "A = vectorizer.fit_transform(raw_documents)\n",
    "print( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (A.shape[0], A.shape[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7807091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def rank_terms( A, terms ):\n",
    "    # get the sums over each column\n",
    "    sums = A.sum(axis=0)\n",
    "    # map weights to the terms\n",
    "    weights = {}\n",
    "    for col, term in enumerate(terms):\n",
    "        weights[term] = sums[0,col]\n",
    "    # rank the terms by their weight over all documents\n",
    "    return sorted(weights.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = rank_terms( A, terms )\n",
    "for i, pair in enumerate( ranking[0:20] ):\n",
    "    print( \"%02d. %s (%.2f)\" % ( i+1, pair[0], pair[1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c3a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "# create the model\n",
    "from sklearn import decomposition\n",
    "model = decomposition.NMF( init=\"nndsvd\", n_components=k ) \n",
    "# apply the model and extract the two factor matrices\n",
    "W = model.fit_transform( A )\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d493bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_term_weights( terms, H, topic_index, top ):\n",
    "    # get the top terms and their weights\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    top_terms = []\n",
    "    top_weights = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( terms[term_index] )\n",
    "        top_weights.append( H[topic_index,term_index] )\n",
    "    # note we reverse the ordering for the plot\n",
    "    top_terms.reverse()\n",
    "    top_weights.reverse()\n",
    "    # create the plot\n",
    "    fig = plt.figure(figsize=(13,8))\n",
    "    # add the horizontal bar chart\n",
    "    ypos = np.arange(top)\n",
    "    ax = plt.barh(ypos, top_weights, align=\"center\", color=\"green\",tick_label=top_terms)\n",
    "    plt.xlabel(\"Term Weight\",fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab917b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_term_weights( terms, H, 1, 15 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06905f25",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee8e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsimodel = LsiModel(corpus=corpus, num_topics=5, id2word=id2word)\n",
    "lsitopics = lsimodel.show_topics(formatted=False)\n",
    "\n",
    "ldatopics = lda_model.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab16df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsitopics = [[word for word, prob in topic] for topicid, topic in lsitopics]\n",
    "\n",
    "hdptopics = [[word for word, prob in topic] for topicid, topic in hdptopics]\n",
    "\n",
    "ldatopics = [[word for word, prob in topic] for topicid, topic in ldatopics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646edaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_coherence = CoherenceModel(topics=lsitopics[:10], texts=data_lemmatized, dictionary=id2word, window_size=10).get_coherence()\n",
    "\n",
    "hdp_coherence = CoherenceModel(topics=hdptopics[:10], texts=data_lemmatized, dictionary=id2word, window_size=10).get_coherence()\n",
    "\n",
    "lda_coherence = CoherenceModel(topics=ldatopics, texts=data_lemmatized, dictionary=id2word, window_size=10).get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c309374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bar_graph(coherences, indices):\n",
    "    assert len(coherences) == len(indices)\n",
    "    n = len(coherences)\n",
    "    x = np.arange(n)\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.bar(x, coherences, width=0.2, tick_label=indices, align='center')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Coherence Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355a3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_bar_graph([lsi_coherence, hdp_coherence, lda_coherence],\n",
    "                   ['LSI', 'HDP', 'LDA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efab952",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "### Would be done via GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca75172",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sujectivity'] = df['lemmatized_comment'].apply(getSubjectivity)\n",
    "df['Polarity'] = df['lemmatized_comment'].apply(getPolarity)\n",
    "df['Analysis'] = df['Polarity'].apply(analysis)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef83c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualization: Sentiment Analysis Plot\n",
    "- Reddit: Sentiment Polarity\n",
    "- Negative, Neutral and Positive\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "K-MEANS Clustering: Polarity vs Subjectivity\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b0a62",
   "metadata": {},
   "source": [
    "# Application & Insights: Next Milestone\n",
    "### Data -> Features -> Model -> Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e30aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='img/roadmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b7513",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Examples: https://dash.gallery/Portal/\\\n",
    "similar project: https://dash.gallery/dash-cytoscape-lda/\n",
    "river: https://github.com/online-ml/river\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45cd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "References:\n",
    "- https://www.kaggle.com/akashram/topic-modeling-intro-implementation/notebook\n",
    "- https://www.kaggle.com/irfanmansuri/nlp-reddit\n",
    "- GPT-3: \n",
    "- Medium: https://medium.com/analytics-vidhya/scraping-reddit-using-python-reddit-api-wrapper-praw-5c275e34a8f4\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f49ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lemmatization: https://www.reddit.com/r/learnpython/comments/ostea3/problem_using_pandas_to_import_excel_sheet_to/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytextrank, spacy\n",
    "import scattertext as st\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "nlp.add_pipe(\"textrank\", last=True)\n",
    "\n",
    "\n",
    "#Dataset \n",
    "convention_df = st.SampleCorpora.ConventionData2012.get_data(\n",
    ").assign(\n",
    "    parse=lambda df: df.text.apply(nlp),\n",
    "    party=lambda df: df.party.apply(\n",
    "        {'disease': 'vaccin', \n",
    "         'covid': 'medication'}.get\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "corpus = st.CorpusFromParsedDocuments(\n",
    "    convention_df,\n",
    "    category_col='party',\n",
    "    parsed_col='parse',\n",
    "    feats_from_spacy_doc=st.PyTextRankPhrases()\n",
    ").build(\n",
    ").compact(\n",
    "    st.AssociationCompactor(2000, use_non_text_features=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa0ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"0\" + \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"hippo\" *12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6acbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb86335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
